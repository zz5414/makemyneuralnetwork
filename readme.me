동그라미 = 0
세모 = 1
네모 = 2

세모가 생각보다 어렵구나 ㅎㅎㅎ
정삼각형을 그리고자 함
이미지는 정삼각형, 정사각형, 정원


이미지 사이즈는 200x200 으로
처음에는 흑백 이미지로 시작
두께는 -1, 1, 2, 3까지
중점과 이미지 사이즈를 입력받아 중점+반지름이 이미지 외곽을 벗어나지 않도록
랜덤하게 그릴 것
training set과 validation set은 달라야 함
training set을 만들 때 training set 전체 데이터의 구성을 csv로 가지고
validation은 이를 피해서 만든다


좀 더 자신이 붙으면 RGB 컬러를 붙여서
색깔을 넣을 예정


색깔을 넣을 수 있다면 배경색도 넣을 예정

처음에는 파일단위로 저장
training 60,000개 생성할 예정
validation 10,000개 생성할 예정

잘 되면 hdf5형태로 저장





1. hdf5 형태로 저장해도 잘 동작하는지 확인



hdf5, CNN, cuda 필요없이 왜 Fully connected layer로도 구분을 못하는지
이유를 알 수가 없음
shutffle을 하지 않아서 학습을 못하던 상황이었음
네모, 원, 세모를 차례대로 보여주니
네모에 피팅.. 원에 피팅.. 세모에 피팅되다가 학습이 끝나는 상황
그러니 세모만 맞추는 classifier가 되어 정확히 0.33의 성능이 나타났던 것

shuffle을 추가하였더니 600개 샘플, 120개 validation에 대해서 97%의 성능
tick=-1을 제거하였고, 이미지의 크기도 140-150으로 제한하였다.(기존 10 ~ 150)

이제 여기서 bounding box까지 맞출 수 있는 시스템을 만든다면 어떨까?


예제코드는 cross_entropy + L1_Loss/1000 로 학습을 하였다.
어떻게 좌표값이 생성되는지는 아직도 의문이다.
cross_entropy가 클래스를 고르기 위한 loss function인것은 어렴풋이 알겠다.
왜 L1_Loss를 쓰고 거기에 1000을 나누어준것일까?
MSE 대신에 L1 Loss를 쓴 것은 취향이고 1000을 나누어준 것은 Cross-entropy와 비슷하게 만들어주기 위함

우선 model을 정의하고 임의의 박스를 그려보는 것으로 시작하자

예제 코드의 cross_entropy가 batch를 기준으로 작성되어 있어
제대로 동작하지 않았다.
class를 prediction하는 부분은 기존과 똑같이 MSE로 하였다.

그랬더니, 클래스 구분력은 67%이지만 박스는 개 똥망이다.(epoch=1기준)

epoch를 2로 올려도 박스는 여전히 작게 인식한다.

박스 loss function을 똑같이 MSE로 하고 나누기 1000하던것을 지웠다.
너무 작게 학습되어서 그런것이 아닐까 추측한다.

(epoch=2)에 78%인식률 과 눈대중으로 IOU ratio 90%를 달성
